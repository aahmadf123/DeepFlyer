## Educational RL Drone Platform Overview

This overview describes the entire project at a high levelâ€”covering user modes, core features, Gazebo simulation designs, speed profiles, hyperparameter options, reward presets (narrative descriptions), and the implementation roadmap. It focuses on **what** will be done rather than **how** to code each piece.

---

### 1. Introduction

An interactive, web-based platform for teaching reinforcement learning (RL) through indoor drone simulations. Inspired by AWS DeepRacer, the platform guides users from basic concepts to advanced experimentation:

* **Explorer Mode** (ages 11â€“22): Provides simple, guided missions with preconfigured reward behaviors, intuitive controls, and step-by-step tutorials. Perfect for newcomers to see how trial and error, rewards, and parameters shape a droneâ€™s learning.
* **Researcher Mode** (advanced): Includes all Explorer features plus:

  * Four extra reward behaviors for deeper experimentation.
  * The ability to upload custom reward logic in Python.
  * Full control over hyperparameters via sliders or text fields.
  * Multi-objective optimization tools to combine different goals.
  * Detailed analytics and visualization of training progress.

**Primary Objectives:**

1. **Fundamental RL Education:** Enable beginners to experiment with trial-and-error, observe the effect of reward shaping, and understand the role of hyperparameters.
2. **In-Depth Research:** Allow advanced users to customize observations, actions, reward functions, and algorithms, then analyze results rigorously.
3. **Indoor Simâ€‘toâ€‘Real Preparation:** Offer a high-fidelity Gazebo + ROSÂ 2 simulation (integrated with PX4/MAVROS), employ domain randomization, and prepare users for eventual transfer to a physical indoor drone.

---

### 2. Gazebo Map Designs

The platform includes three indoor Gazebo worlds of increasing complexity, each targeting different RL skills:

1. **Map 1: Simple Course**
   â€¢ A straight, wide corridor (10â€¯m Ã—â€¯10â€¯m floor, 3â€¯m ceiling) with a few static obstacles (boxes or cylinders).
   â€¢ Teaches basic waypoint navigation and altitude holding.
   â€¢ Variations: gentle wind gusts, a slow-moving obstacle, and an altitude-hold subtask.

2. **Map 2: Multiâ€‘Path Complex**
   â€¢ A 10â€¯m Ã—â€¯10â€¯m area divided into three route options: one narrow but short, one wide but long, and one medium with obstacles.
   â€¢ Introduces dynamic barriers (gates that open/close) and localized wind zones.
   â€¢ Encourages path planning, adaptive route selection, and improved collision avoidance under changing conditions.

3. **Map 3: Multiâ€‘Level Challenging**
   â€¢ A 10â€¯m Ã—â€¯10â€¯m base footprint with three vertical levels connected by ramps and floating platforms. Some zones are â€œno-flyâ€ below certain heights.
   â€¢ Features narrow passages, time-based gates (open for limited intervals), moving platforms, and dynamic lighting conditions.
   â€¢ Challenges the agent to navigate between levels, land on specific platforms, avoid moving obstacles, and manage altitude changes.

> *Note:* Although each map fits within a 10â€¯m Ã—â€¯10â€¯m Ã—â€¯3â€¯m boundary, the complexity increases by adding vertical elements, dynamic obstacles, and environment variations.

---

### 3. Speed Profiles

Controlling flight speed helps balance exploration and stability. Two sets of profiles address different user needs:

* **Explorer Mode (3 Presets):**
  â€¢ **Slow & Safe:** Maximum speed 0.5â€¯m/s, acceleration 1â€¯m/sÂ²â€”ideal for cautious beginners.
  â€¢ **Normal:** Maximum speed 1â€¯m/s, acceleration 2â€¯m/sÂ²â€”a balanced default.
  â€¢ **Fast:** Maximum speed 1.5â€¯m/s, acceleration 3â€¯m/sÂ²â€”for users seeking quicker progress at some risk.

* **Researcher Mode (Custom Sliders):**
  â€¢ Velocity slider from 0.2â€¯m/s to 2.0â€¯m/s.
  â€¢ Acceleration slider from 0.5â€¯m/sÂ² to 5.0â€¯m/sÂ².
  â€¢ Enables fine-grained experiments exploring trade-offs between speed, stability, and learning efficiency.

---

### 4. Hyperparameter Options

#### 4.1 Explorer Mode Hyperparameters (Predefined Choices)

* **Learning Rate:** Select among 0.0001, 0.0003, or 0.001.
* **Total Episodes:** Choose 100, 500, or 1000 episodes of training.
* **Batch Size:** Select 32, 64, or 128 transitions per training update.

These options simplify tuning for beginners by limiting the range to sensible defaults.

#### 4.2 Researcher Mode Hyperparameters (Full Control)

* **Learning Rate Slider:** Range from 0.00001 to 0.01, allowing fine adjustments.
* **Discount Factor (Î³) Slider:** From 0.90 to 0.999, controlling how future rewards are valued.
* **Entropy Coefficient Slider:** From 0 to 0.1, adjusting exploration vs. exploitation.
* **Value Loss Coefficient Slider:** From 0.5 to 1.0, weighting the value-function loss term.
* **Batch Size Selector:** Any power-of-two between 16 and 512.
* **PPO Clip (Îµ) Slider:** From 0.1 to 0.3, controlling trust-region size.
* **Max Gradient Norm Slider:** From 0.1 to 1.0, setting gradient clipping thresholds.
* **Epochs per Update Choices:** 3, 5, 7, or 10 pass-throughs over each batch.
* **Replay Buffer Size Options:** If using off-policy methods, choose among 10â€¯k, 25â€¯k, 50â€¯k, or 100â€¯k samples.

> *Note:* Researchers can hide individual hyperparameter fields behind a single â€œdifficultyâ€ slider if desired, allowing novices to still benefit from tuned defaults.

---

### 5. Core Parameter Categories & Details

#### 5.1 Environment & Simulation Parameters

* **Map Dimensions & Obstacles:** All maps remain inside a 10â€¯m Ã—â€¯10â€¯m Ã—â€¯3â€¯m volume. Static boxes or cylinders are placed manually; dynamic barriers and platforms move according to simple scripts.
* **Lighting & Shadows:** Indoor lighting levels vary between 200 and 800Â lux; shadows can be toggled to simulate different realâ€world room conditions.
* **Physics Settings:** Standard Earth gravity (9.81â€¯m/sÂ²), drag coefficient in the range 0.1â€‰â€“â€‰0.3 for aerodynamic damping, zero restitution on collisions (no bounce), and a floor friction coefficient of 0.5.
* **Disturbances & Randomization:** Optional wind gusts of 0.1â€‰â€“â€‰0.3â€¯m/s can be enabled in localized zones. Random force noise up to Â±0.01â€¯N is applied per timestep to simulate subtle indoor air currents. Sensor noise (Gaussian) is added to camera images (Ïƒâ€‰=â€‰5 intensity levels), IMU accelerometer readings (Ïƒâ€‰=â€‰0.02â€¯m/sÂ²) and gyroscope readings (Ïƒâ€‰=â€‰0.01â€¯rad/s), and depth sensor measurements (Â±0.05â€¯m) to improve robustness.

#### 5.2 State / Observation Parameters

* **Sensor Update Rates:** Front camera publishes at 15â€¯fps; downward camera at 10â€¯fps; IMU at 100â€¯Hz; SLAM pose updates at 10â€¯Hz.
* **Camera Specifications:** 640â€¯Ã—â€¯480-pixel resolution with a 90Â°Â horizontal FOV and 60Â°Â vertical FOV.
* **SLAM Configuration:** Occupancy grid resolution of 0.05â€¯m per cell, pose noise tolerance of 0.1â€¯m, and scan-match tolerance of 0.05â€¯m.
* **Observation Vector Contents:** Includes 3D position and orientation (quaternion), linear and angular velocities, a processed depth slice or point-cloud segment, a downsampled front-camera image, a collision flag or nearest-obstacle distance, and an optional battery/energy state estimate.

#### 5.3 Action / Control Parameters

* **Maximum Velocities & Accelerations:** Linear velocity capped at 1.5â€¯m/s, linear acceleration at 2.0â€¯m/sÂ²; angular velocity capped at Ï€/2â€¯rad/s, angular acceleration at Ï€â€¯rad/sÂ².
* **Control Mode:** The agent sends velocity commands (`/cmd_vel`) combining linear and angular components. A PID controller (Kâ‚šÂ =Â 0.5, Káµ¢Â =Â 0.1, Kğ’¹Â =Â 0.05) ensures smooth tracking.
* **Action-Space Representation:** Explorer uses a small discrete set (e.g., five bins for each linear axis and three for yaw) to simplify decision-making. Researchers can work with a continuous 4D action vector \[vâ‚“, váµ§, v\_z, Ï‰\_yaw] bounded by the above limits.

#### 5.4 Reward Function Parameters

* **Distance-to-Goal Component:** Encourages moving toward a specified waypoint. The closer the drone gets, the higher the reward, with full reward at within 0.2â€¯m of the goal.
* **Collision Penalty:** A heavy negative penalty (â€“10) for any collision. If within 0.5â€¯m of an obstacle, a smaller penalty is applied, increasing as distance decreases; at under 0.2â€¯m, a near-miss penalty (â€“1) is assigned.
* **Smoothness (Jerk) Penalty:** Penalizes rapid changes in acceleration (jerk). If linear jerk exceeds 0.5â€¯m/sÂ³ or angular jerk exceeds 0.5â€¯rad/sÂ², the reward is reduced proportionally, up to a 30â€¯% penalty weight.
* **Energy (Throttle) Penalty:** Penalizes throttle usage when above 0.7 (70â€¯% power), encouraging energy-efficient flight. A weight of 0.2 is used for this component.
* **Time/Completion Bonus:** A small time-based penalty (â€“0.01 per timestep) encourages faster completion. Upon reaching the goal, a completion bonus of +5 is awarded.
* **Multi-Objective Weights:** In Researcher Mode, users assign weights to each componentâ€”distance, collision avoidance, energy usage, and speedâ€”so they can tune trade-offs dynamically.

#### 5.5 Training & Episode Parameters

* **Maximum Episodes:** 1000 episodes of training are allowed per run, giving ample opportunity for convergence.
* **Steps per Episode:** Each episode lasts up to 500 steps (0.05â€¯s per step, totaling 25â€¯s max). Episodes terminate early if the goal is reached or a crash occurs.
* **Warm-Up Steps:** The first 1000 steps are random actions (no learning) to fill any replay buffer and avoid cold starts.
* **Evaluation Frequency:** Every 50 training episodes, the agent runs 5 evaluation episodes with no exploration noise to track progress.
* **Checkpoint Frequency:** The model is saved every 100 episodes for later analysis or rollback.

#### 5.6 Curriculum Mode Parameters

* **Scenario Sequence:** Training proceeds through three sequential worldsâ€”MapÂ 1 (simple), then MapÂ 2 (complex), then MapÂ 3 (multi-level).
* **Episode Thresholds to Advance:** To progress from one map to the next, the average reward over the last 20 episodes must exceed 50.
* **Reward Thresholds per Map:** During MapÂ 1, the threshold is 30; MapÂ 2 requires 40; MapÂ 3 requires 50.
* **Reset Conditions:** If the drone crashes three consecutive times, the current scenario resets to avoid wasted training time.

#### 5.7 XAI & AI Coach Parameters

* **Saliency Overlay Threshold:** Activations above 0.6 are visualized to show which camera pixels the network attends to.
* **Coach Plateau Length:** If there is <5â€¯% improvement in reward over 50 episodes, the AI Coach provides tips (e.g., adjust learning rate or modify reward weights).
* **Collision Rate Threshold:** If crashes exceed 20â€¯% of episodes over a 20-episode window, the Coach suggests stronger collision penalties or reduced speed.
* **Max Suggestion Frequency:** The Coach offers at most one suggestion every 20 episodes to avoid overwhelming users.

---

### 6. Gazebo Map Designs (Summary)

1. **MapÂ 1: Simple Course**
   â€¢ Straight corridor with a few static boxes; target altitude hold at 1.5â€¯m; optional wind gusts.
2. **MapÂ 2: Multi-Path Complex**
   â€¢ Three possible routes (short/narrow, long/open, medium/obstacles), dynamic gates, localized wind zones.
3. **MapÂ 3: Multi-Level Challenging**
   â€¢ Three stacked levels, moving platforms, timed gates, no-fly zones, variable lighting.

Each world provides the agent with progressively harder tasks, focusing on navigation, path planning, altitude control, and timed interactions.

---

### 7. Speed Profiles (Summary)

* **Explorer Mode:**
  â€¢ *Slow & Safe* (0.5â€¯m/s,Â 1.0â€¯m/sÂ²)
  â€¢ *Normal* (1.0â€¯m/s,Â 2.0â€¯m/sÂ²)
  â€¢ *Fast* (1.5â€¯m/s,Â 3.0â€¯m/sÂ²)
* **Researcher Mode:**
  â€¢ *Custom Sliders* allow velocity from 0.2â€¯m/s to 2.0â€¯m/s and acceleration from 0.5â€¯m/sÂ² to 5.0â€¯m/sÂ² for precise experimentation.

---

### 8. Hyperparameter Options (Summary)

* **Explorer Mode:**
  â€¢ Learning Rate: {0.0001,Â 0.0003,Â 0.001}
  â€¢ Episodes: {100,Â 500,Â 1000}
  â€¢ Batch Size: {32,Â 64,Â 128}
* **Researcher Mode:**
  â€¢ Learning Rate slider (0.00001â€“0.01)
  â€¢ Discount (Î³) slider (0.90â€“0.999)
  â€¢ Entropy Coefficient slider (0â€“0.1)
  â€¢ Value Loss Coefficient slider (0.5â€“1.0)
  â€¢ Batch Size selector (16â€“512)
  â€¢ PPO Clip (Îµ) slider (0.1â€“0.3)
  â€¢ Max Gradient Norm slider (0.1â€“1.0)
  â€¢ Epochs/Update choices {3,Â 5,Â 7,Â 10}
  â€¢ Replay Buffer size options {10â€¯k,Â 25â€¯k,Â 50â€¯k,Â 100â€¯k}

---

### 9. Implementation Roadmap (High-Level)

1. **WeeksÂ 1â€“2 (Environment & Setup)**

   * Set up ROSÂ 2 & Gazebo.
   * Create a basic URDF with camera, IMU, and collision tags.
   * Scaffold the FastAPI backend and React frontend stubs.
   * Define the first six reward behaviors in a registry.

2. **WeeksÂ 3â€“4 (Basic Training Loop)**

   * Add motor and velocity command support, verify hover and basic navigation.
   * Connect the frontend â€œStart Trainingâ€ button to backend endpoints.
   * Implement a baseline PPO training loop using distance-to-goal rewards.
   * Log training metrics to a file or database.

3. **WeeksÂ 5â€“6 (Sensor & Reward Extensions)**

   * Integrate PX4/MAVROS for realistic drone flight in Gazebo.
   * Build a simple SLAM pipeline using the downward camera for altitude and positioning.
   * Add energy-efficiency and smoothness reward behaviors, test them in MapÂ 1 and MapÂ 2.
   * Validate collision avoidance in a dynamic obstacle scenario.

4. **WeeksÂ 7â€“8 (Analytics & XAI)**

   * Develop an evaluation dashboard displaying reward vs. episode curves, crash statistics, and variance.
   * Implement Grad-CAMâ€“based saliency maps to visualize what the agent â€œlooks atâ€ during flight.
   * Add hyperparameter sliders to the UI and an â€œauto-tune assistantâ€ that suggests adjustments when progress plateaus.

5. **WeeksÂ 9â€“10 (Curriculum & Scenario Creator)**

   * Complete MapÂ 3 (multi-level) and add RL tutorial overlays explaining concepts in-context.
   * Build a drag-and-drop scenario creator allowing users to place obstacles and define waypoints.
   * Implement curriculum training: automatically progress from MapÂ 1 to MapÂ 3 based on reward thresholds.

6. **WeekÂ 11 (Advanced Rewards & Custom Uploader)**

   * Introduce three advanced reward behaviors (path efficiency, adaptive disturbance, multi-objective).
   * Add a UI component for uploading custom Python reward code and validating its format.
   * Enable dynamic weighting of multi-objective components in the UI.

7. **WeekÂ 12 (Domain Randomization & Simâ€‘toâ€‘Real)**

   * Enable sensor noise, random wind, and lighting variations in all Gazebo maps.
   * Finalize model export to ONNX format.
   * Provide a â€œSimâ€‘toâ€‘Realâ€ script outline for testing on a physical drone or staging simulator.
   * Publish a concise â€œSimâ€‘toâ€‘Realâ€ checklist and tutorial for future hardware integration.

---

### 10. Reward Preset Descriptions (No Code)

#### 10.1 Explorer Mode Presets (6)

1. **Reach the Target:**
   Encourage the drone to move toward a specified waypoint. Rewards increase as the drone gets closer, with maximum reward when within 0.2â€¯m of the goal.

2. **Avoid Crashes:**
   Penalize collisions heavily. If the drone comes within 0.5â€¯m of an obstacle, it receives a small penalty that grows as the distance decreases. A near-miss under 0.2â€¯m incurs a moderate penalty.

3. **Save Energy:**
   Encourage low throttle usage. The less throttle (motor power) the drone applies, the higher the rewardâ€”promoting energy-efficient flight for longer missions.

4. **Fly Steady:**
   Reward the drone for maintaining a constant target altitude. Deviations from the target altitude reduce the reward proportionally, and vertical velocity incurs an additional small penalty.

5. **Fly Smoothly:**
   Penalize sudden changes in acceleration (jerk) or rapid yaw rotations. The smoother the velocity and turning profiles, the higher the rewardâ€”ideal for inspection or cinematography tasks.

6. **Be Fast:**
   Reward rapid mission completion. Each timestep carries a small negative reward to incentivize speed. When the drone reaches the goal, it receives a completion bonus scaled by how quickly it arrived.

#### 10.2 Researcher Mode Additional Presets (3)

7. **Path Efficiency:**
   Compare the total distance traveled to the straight-line distance between start and goal. Agents that take near-straight paths receive higher rewards; winding or unnecessarily long routes yield lower scores.

8. **Adaptive Disturbance Compensation:**
   Simulate small indoor disturbances (e.g., subtle drafts). Reward the drone for applying thrust that counters these random forces. Effective disturbance compensation leads to higher reward, while letting wind push you off course incurs penalties.

9. **Multi-Objective Optimization:**
   Combine multiple reward componentsâ€”such as reaching the target, avoiding collisions, saving energy, and maximizing speedâ€”each weighted by a user-defined parameter. This preset allows advanced users to study trade-offs and find Pareto-optimal strategies.

---

*This updated overview now fully covers the platformâ€™s scopeâ€”Gazebo map designs, speed profiles, hyperparameter options, reward presets (described narratively), and a high-level implementation roadmapâ€”without any code blocks, ensuring readability and clarity for all stakeholders.*
