## 12-Week Summer Plan (May 14 ‚Äì July 27, 2025)

Below is a concise, Markdown-friendly table showing each team member's responsibilities over the 12-week period. Roles: **Uma (Simulation & CAD)**, **Jay (UI & Backend Integration)**, **Ahmad [Me] (RL & AI)**.

| Week | Dates        | Uma: Simulation & CAD                                                                                                                                                 | Jay: UI & Backend Integration                                                                                                                                                                                                                                                 | Ahmad: RL & AI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| :--: | :----------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|   1  | May 14‚Äì20    | ‚Ä¢ Set up ROS 2 & Gazebo environment.<br>‚Ä¢ Create bare-bones URDF visual model (no sensors).<br>‚Ä¢ Verify drone spawns in Gazebo.                                       | ‚Ä¢ Scaffold FastAPI backend with placeholder endpoints.<br>‚Ä¢ Initialize React/Next.js project skeleton.<br>‚Ä¢ Define empty MongoDB schema.<br>‚Ä¢ Create a stub "Training" page.                                                                                                  | ‚Ä¢ Install Python, PyTorch + CUDA, RL libraries.<br>‚Ä¢ Clone simulation repo and verify ROS 2 topics.<br>‚Ä¢ Start a Python package for the RL agent with a basic P3O skeleton.<br>‚Ä¢ Implement a "hello-world" loop sending zero velocities and logging status.                                                                                                                                                                                                                                                                                     |
|   2  | May 21‚Äì27    | ‚Ä¢ Add IMU & collision tags to URDF.<br>‚Ä¢ Attach front-facing camera plugin (publish to `/drone/camera/front/image_raw`).<br>‚Ä¢ Optimize meshes and validate in Gazebo. | ‚Ä¢ Expose `/api/rewards/list` and `/api/train/start`, `/api/train/status` (dummy).<br>‚Ä¢ Build Mission Selector UI stub: dropdown for reward presets, "Start Training" button. | ‚Ä¢ Define simplified two-term reward approach: `follow_trajectory` (cross-track error) and `heading_error`.<br>‚Ä¢ Implement a `RewardRegistry` mapping IDs ‚Üí functions, including metadata (friendly names, descriptions).<br>‚Ä¢ Begin designing the custom-reward sandbox & validation framework (signature checks, safe execution).<br>‚Ä¢ Expose `RewardRegistry.list_presets()` to FastAPI. |
|   3  | May 28‚ÄìJun 3 | ‚Ä¢ Add motor plugins so drone can fly under velocity commands.<br>‚Ä¢ Validate hover via Gazebo's GUI.                                                                   | ‚Ä¢ Populate dropdown by calling `/api/rewards/list`.<br>‚Ä¢ On "Start Training," POST preset & defaults to `/api/train/start`.<br>‚Ä¢ Show spinner awaiting `/api/train/status`.<br>‚Ä¢ Set up WebSocket skeleton for future streaming.                                              | ‚Ä¢ Implement baseline P3O training loop:<br>  ‚Äì Subscribe to `/drone/odom` and `/drone/camera/front/image_raw`.<br>  ‚Äì Publish actions to `/drone/cmd_vel`.<br>  ‚Äì Reward = ‚Äìdistance_to_goal.<br>  ‚Äì Update every 64 steps.<br>  ‚Äì Refactor logging into a unified metrics module (standardized JSON schema for reward breakdown and training stats) and log metrics to CSV/JSON ready for live streaming. |
|   4  | Jun 4‚Äì10     | ‚Ä¢ Test simple "move to waypoint" scenario in Gazebo.<br>‚Ä¢ Add downward-facing camera plugin for SLAM later.                                                           | ‚Ä¢ Connect `/api/train/start` to spawn background training job.<br>‚Ä¢ Return `job_id` & set status to "started."<br>‚Ä¢ Implement `/api/train/status` by reading JSON status file.                                                                                                | ‚Ä¢ Refactor to support `path_efficiency` reward.<br>‚Ä¢ Accept `preset_id` from Jay's UI and use corresponding function.<br>‚Ä¢ Validate that `path_efficiency_reward` changes logged rewards meaningfully.                                                                                                                                                                                                                                                                                                                                          |
|   5  | Jun 11‚Äì17    | ‚Ä¢ Integrate PX4/MAVROS so Gazebo simulates real autopilot behavior.<br>‚Ä¢ Finalize Mission Selector UI readiness.                                                      | ‚Ä¢ Build "Simulation Viewer": embed Gazebo camera feed via WebSocket or MJPEG.<br>‚Ä¢ Add sliders for max velocity & acceleration, send to `/api/train/start`.                                                                                                                   | ‚Ä¢ Subscribe to downward camera, run simple SLAM (ORB-SLAM2 wrapper) to get altitude or 2D map.<br>‚Ä¢ Implement `energy_efficiency_reward` penalizing throttle usage.<br>‚Ä¢ Compare energy-efficiency vs. path-efficiency in small experiments.                                                                                                                                                                                                                                                                                                    |
|   6  | Jun 18‚Äì24    | ‚Ä¢ Debug MAVROS communication and data routing to ROS topics.<br>‚Ä¢ Ensure collision detection works accurately.                                                        | ‚Ä¢ Integrate front camera stream into dashboard.<br>‚Ä¢ Create a "Telemetry Overlay" showing episode number, last reward, `preset_id`.<br>‚Ä¢ Route hyperparameter sliders to `/api/train-start`.                                                                                  | ‚Ä¢ Route training parameters (e.g. collision penalty weight) to ROS topics so simulation can use them.<br>‚Ä¢ Implement & validate `collision_avoidance_reward` and `fly_smoothly_reward`.<br>‚Ä¢ Run brief tests to confirm correct behavior.                                                                                                                                                                                                                                                                                                       |
|   7  | Jun 25‚ÄìJul 1 | ‚Ä¢ Validate full URDF + PX4 + sensor stack under new presets.<br>‚Ä¢ Build Gazebo world 2 (Map 2: multi-path complex with dynamic obstacles).                            | ‚Ä¢ Add real-time charts: episode reward vs. episode, crash counts, variance.<br>‚Ä¢ Enable side-by-side comparison of two training jobs' reward curves.                                                                                                                          | ‚Ä¢ Build interactive hyperparameter UI component:<br>  ‚Äì Sliders for `learning_rate`, `gamma`, `entropy_coef`.<br>  ‚Äì On change, send new values to `/api/train/start`.<br>‚Ä¢ Implement the core auto-tune assistant: monitor reward plateaus and crash rates, prototype grid/Bayesian search over key hyperparameters, and output structured suggestions to JSON for Jay's UI.                                                                                                                                                                                                                                                |
|   8  | Jul 2‚Äì8       | ‚Ä¢ Finish Map 2: three route options, dynamic barriers, wind zones.<br>‚Ä¢ Test SLAM & collision avoidance in Map 2.                                                     | ‚Ä¢ Develop "Evaluation Dashboard":<br>  ‚Äì Graph: reward vs. episode with annotations.<br>  ‚Äì Metrics: standard deviation of rewards, crashes per 10 episodes.<br>  ‚Äì Flight-replay mini-map colored by speed or confusion.                                                     | ‚Ä¢ Integrate XAI overlays:<br>  ‚Äì Compute Grad-CAM saliency from P3O's CNN layers.<br>  ‚Äì Overlay onto camera stream and publish via WebSocket.<br>  ‚Äì Allow toggling "Show Saliency" in UI.<br>‚Ä¢ Instrument training loop to emit per-step and per-episode reward breakdown (distance, collision, energy components) in JSON for the frontend.                                                                                                                                                                                                                                                                                     |
|   9  | Jul 9‚Äì15     | ‚Ä¢ Build Map 3: multi-level complex (vertical levels, floating platforms, dynamic lighting).<br>‚Ä¢ Validate SLAM & sensors on Map 3.                                    | ‚Ä¢ Create an "RL Glossary" hover component: definitions for "episode," "reward," "policy."<br>‚Ä¢ Add tooltips next to hyperparameter sliders (e.g. explain "learning rate" in plain language).<br>‚Ä¢ Integrate saliency stream so users can toggle it on/off.                    | ‚Ä¢ Expand RL tutorial overlays:<br>  ‚Äì Show "Agent is exploring" indicator when entropy high.<br>  ‚Äì Refine AI Coach: analyze training logs (reward, loss, entropy) and trigger contextual tips (e.g. "lower learning rate" or "reduce collisions at Platform 2").<br>  ‚Äì Expose `/api/coach/suggestions` for Jay's UI.                                                                                                                                                                                                                          |
|  10  | Jul 16‚Äì22     | ‚Ä¢ Draft Curriculum Mode: three scenarios in sequence (Maps 1 ‚Üí 2 ‚Üí 3).<br>‚Ä¢ Configure Gazebo to load scenarios sequentially based on triggers.                        | ‚Ä¢ Build "Scenario Creator" UI:<br>  ‚Äì Drag & drop obstacles onto blank grid.<br>  ‚Äì Set waypoints and checkpoints.<br>  ‚Äì Save JSON to `/api/scenario/upload`.<br>‚Ä¢ Create "Curriculum" page where users pick scenario sequence.                                              | ‚Ä¢ Implement `ScenarioLoader`:<br>  ‚Äì Read JSON from `/api/scenario/list`.<br>  ‚Äì Spawn/Delete models via ROS 2 services.<br>  ‚Äì Load chosen scenario at training start.<br>‚Ä¢ Create `CurriculumRunner` that:<br>  1. Trains on Scenario 1 until reward threshold.<br>  2. Saves model, loads Scenario 2, resumes training.<br>  3. Repeats for Scenario 3.<br>‚Ä¢ Test end-to-end with `curriculum=true` flag.<br>‚Ä¢ Harden the CurriculumRunner: add checkpointing, early-stop criteria, failure recovery, and model rollback between scenarios. |
|  11  | Jul 23‚Äì29     | ‚Ä¢ Validate multi-objective rewards in Map 3 under Curriculum Mode.<br>‚Ä¢ Add necessary sensors/triggers for multi-objective tasks.                                     | ‚Ä¢ Add UI controls for multi-objective weights:<br>  ‚Äì Sliders for path_efficiency, collision_avoidance, energy_saving, speed.<br>  ‚Äì Send weight vector to `/api/train/start`.<br>‚Ä¢ Build "Custom Reward Function" uploader: upload Python script, call Ahmad's validator. | ‚Ä¢ Implement `multi_objective_reward(state, action, weights)`:<br>  ‚Äì Weighted sum of `reach_target`, `avoid_crashes`, `save_energy`, and step-wise speed.<br>  ‚Äì Normalize each component.<br>‚Ä¢ Implement `dynamic_mission_reward` to handle mid-mission goal changes.<br>‚Ä¢ Write a "reward validator" script that:<br>  1. Loads the user's uploaded Python file and checks for signature `def custom_reward(state, action) -> float`.<br>  2. Runs a few dummy state/action tests.<br>  3. Returns pass/fail to Jay's `/api/reward/validate`.<br>‚Ä¢ Implement `adaptive_disturbance_reward` and an `intrinsic_motivation_reward` preset for novelty-based exploration. |
|  12  | Jul 30‚ÄìAug 5  | ‚Ä¢ Finalize domain randomization in Gazebo (vary wind, lighting, sensor noise).<br>‚Ä¢ Create "Sim-to-Real" checklist PDF.                                               | ‚Ä¢ Build "Sim-to-Real" tutorial page:<br>  ‚Äì Embed checklist steps.<br>  ‚Äì Provide "Download Model" button to fetch final ONNX file.<br>  ‚Äì Offer "Dry-Run in Simulation" feature for final validation.                                                                        | ‚Ä¢ Integrate domain-randomization callbacks into the RL training loop to sample new sensor and force noise levels each episode.<br>‚Ä¢ Complete domain randomization:<br>  ‚Äì Randomize IMU noise, camera noise, minor force disturbances per episode.<br>  ‚Äì Confirm models generalize across all three Gazebo worlds.<br>  ‚Äì Export final P3O model to ONNX and test loading in a lightweight runner.<br>‚Ä¢ Write `sim_to_real_runner.py` that:<br>  1. Loads ONNX model.  2. Connects to real drone's ROS 2 topics or staging simulator.  3. Streams telemetry and logs behavior differences.<br>‚Ä¢ Deliver "Sim-to-Real" documentation for hardware team.                                                                      |

---

## Key Parameter Categories & Specific Parameters

Below are the **parameter categories** and **specific parameters** you'll need to define for an indoor-only RL setup.

### 1. Environment & Simulation Parameters

*(Primarily configured by Uma in Gazebo; Jay exposes toggles/fields but does not handle the physics directly.)*

* **Map Dimensions & Boundaries**

  * *Floor plan size (X √ó Y)*: 10 m √ó 10 m
  * *Ceiling height*: 3 m
  * *Obstacle density*: 0.1 obs/m¬≤
  * *Obstacle shapes & positions*: e.g. (box, cylinder), coordinates in meters

* **Lighting Conditions**

  * *Ambient light intensity range*: 200‚Äì800 lux
  * *Shadow variation*: boolean (on/off) or intensity parameter

* **Physics & Collision**

  * *Gravity*: 9.81 m/s¬≤
  * *Air friction/damping*: drag coefficient 0.1‚Äì0.3
  * *Collision restitution (bounciness)*: 0.0 for hard collisions
  * *Floor friction coefficient*: 0.5

* **Wind & External Disturbances** (optional indoors)

  * *Wind gain*: 0 (no wind) or small gusts (0.1‚Äì0.3 m/s)
  * *Random force noise magnitude*: ¬±0.01 N per timestep

* **Sensor Noise / Domain Randomization**

  * *Camera Gaussian noise œÉ*: e.g. 5 intensity levels
  * *IMU noise floor*: accel œÉ=0.02 m/s¬≤, gyro œÉ=0.01 rad/s
  * *Depth-sensor noise*: ¬±0.05 m
  * *Randomization ranges*: e.g. IMU noise ‚àº U(0.01, 0.03)

### 2. State / Observation Parameters

*(Ahmad's RL code reads these; Uma publishes them; Jay may show them as "observation info" in the UI.)*

* **Sensor Frame Rates**

  * *Camera FPS*: 15 fps (front), 10 fps (downward)
  * *IMU update rate*: 100 Hz
  * *SLAM update rate*: 10 Hz

* **Camera Resolution & Field of View**

  * *Resolution (W √ó H)*: 640 √ó 480 pixels
  * *FOV horizontal*: 90¬∞
  * *FOV vertical*: 60¬∞

* **SLAM / Localization Outputs**

  * *Map resolution*: 0.05 m per grid cell
  * *Pose noise threshold*: 0.1 m
  * *Scan match tolerance*: 0.05 m

* **Observation Vector Contents**

  * Position (x, y, z) and orientation (quaternion)
  * Linear & angular velocities (IMU data)
  * Depth or point-cloud slice from ToF/LiDAR
  * Front camera image (downsampled or grayscale)
  * Collision flag or distance_to_obstacle
  * Battery or energy estimate (optional)

### 3. Action / Control Parameters

*(Ahmad's RL loop writes these; Uma must accept them in Gazebo; Jay provides UI fields for end-users.)*

* **Velocity & Acceleration Limits**

  * *Max linear velocity (m/s)*: 1.5 m/s
  * *Max linear acceleration (m/s¬≤)*: 2.0 m/s¬≤
  * *Max angular velocity (rad/s)*: œÄ/2 rad/s
  * *Max angular acceleration (rad/s¬≤)*: œÄ rad/s¬≤

* **Control Mode**

  * *Velocity vs. attitude commands*: use `/cmd_vel` (linear & angular velocities)
  * *PID gains for velocity loop*: K‚Çö=0.5, K·µ¢=0.1, Kùíπ=0.05

* **Action Discretization vs. Continuous**

  * *Discrete example*:
    ¬†¬†¬†¬†‚Ä¢ 5 bins for linear x/y/z
    ¬†¬†¬†¬†‚Ä¢ 3 bins for yaw (left, none, right)
  * *Continuous example*:
    ¬†¬†¬†¬†‚Ä¢ Action vector ‚àà ‚Ñù‚Å¥: \[v‚Çì, v·µß, v_z, œâ_yaw], with bounds as above

### 4. Reward Function Parameters

*(Ahmad's RL code reads these; Jay shows sliders/inputs for user tuning; Uma ensures simulation publishes required state values.)*

* **Path Following Component (Cross-Track Error)**

  * *cross_track_weight*: 1.0
  * *max_error*: 2.0 m

* **Heading Error Component**

  * *heading_weight*: 0.1
  * *max_heading_error*: œÄ rad

* **Two-Term Reward Function**

  * Simple, educational approach combining path following and heading alignment
  * Normalized components for consistent scaling
  * Easy to understand and tune for beginners

### 5. RL Hyperparameters

*(Ahmad sets defaults; Jay provides sliders/fields for user tuning.)*

* **Learning Rate (Œ±)**: 1e-5 ‚Üí 1e-2 (default 3e-4)
* **Discount Factor (Œ≥)**: 0.90 ‚Üí 0.999 (default 0.99)
* **Batch Size**: {32, 64, 128, 256}
* **Clip (P3O Œµ)**: 0.1 ‚Üí 0.3 (default 0.2)
* **Entropy Coefficient**: 0.0 ‚Üí 0.1 (default 0.01)
* **Value Loss Coefficient (œ∞_v)**: 0.5 ‚Üí 1.0 (default 0.5)
* **Epochs per Update**: 3 ‚Üí 10 (default 5)
* **Max Gradient Norm**: 0.1 ‚Üí 1.0 (default 0.5)
* **Replay Buffer Size** (if off-policy): 10k ‚Üí 100k (default 50k)

### 6. Training & Episode Parameters

*(Ahmad's code uses these; Jay provides fields.)*

* **Max Episodes**: 1000
* **Max Steps/Episode**: 500 (‚àÜt=0.05 s, total 25 s)
* **Warm‚ÄêUp (random actions)**: 1000 steps
* **Evaluation Frequency**: every 50 episodes (evaluate 5 episodes)
* **Checkpoint Frequency**: every 100 episodes

### 7. Curriculum Mode Parameters

*(Researcher only; UI fields generate a JSON list.)*

* **Scenario Sequence**: \["map_simple", "map_complex", "map_multilevel"]
* **Episode thresholds to progress**: average reward ‚â• 50 over last 20 episodes
* **Reward thresholds**: \[30, 40, 50] per scenario
* **Reset condition**: 3 crashes in a row

### 8. XAI & AI Coach Parameters

*(Researcher only; relevant to advanced users.)*

* **Saliency overlay threshold**: 0.6 activation
* **Coach plateau length**: 50 episodes with <5 reward improvement
* **Collision rate threshold**: >20% crashes over 20 episodes
* **Max suggestion frequency**: once every 20 episodes

---

## Preset Reward Functions

Below are the full definitions (in Python‚Äêstyle pseudocode) of each preset reward, grouped by mode.

### Explorer Mode (6 Presets)

1. **`reach_target_reward`**

   ```python
   def reach_target_reward(state, action):
       # state["position"] = (x, y, z)
       # state["goal"]     = (gx, gy, gz)
       # state["max_room_diagonal"] = diagonal length (precomputed)
       dist = euclidean_distance(state["position"], state["goal"])
       # Reward ‚àà [0, 1], with 1 at the goal, falls off linearly
       return max(0.0, 1.0 - (dist / state["max_room_diagonal"]))
   ```

2. **`avoid_crashes_reward`**

   ```python
   def avoid_crashes_reward(state, action):
       # state["collision_flag"]    = True/False
       # state["dist_to_obstacle"]  = nearest obstacle distance (m)
       if state["collision_flag"]:
           return -1.0  # heavy penalty for collision
       elif state["dist_to_obstacle"] < 0.2:
           return -0.5  # moderate penalty for very close
       else:
           return 0.0   # no penalty otherwise
   ```

3. **`save_energy_reward`**

   ```python
   def save_energy_reward(state, action):
       # action["throttle"] ‚àà [0.0, 1.0]
       throttle = action["throttle"]
       # Reward is high if throttle is low; 1 (best) at 0, 0 (worst) at 1
       return 1.0 - throttle
   ```

4. **`fly_steady_reward`**

   ```python
   def fly_steady_reward(state, action):
       # state["altitude"]          = current z
       # state["target_altitude"]   = desired z
       # state["vertical_velocity"] = current vz
       # state["max_altitude_error"] = e.g. 1.0 m (normalization)
       alt_err = abs(state["altitude"] - state["target_altitude"])
       vz = abs(state["vertical_velocity"])
       # Baseline: 1.0 ‚Äì (alt_error normalized), then penalize vertical speed
       altitude_component = max(0.0, 1.0 - (alt_err / state["max_altitude_error"]))
       speed_penalty = 0.5 * vz
       return altitude_component - speed_penalty
   ```

5. **`fly_smoothly_reward`**

   ```python
   def fly_smoothly_reward(state, action):
       # state["prev_velocity"] (vx_prev, vy_prev, vz_prev)
       # state["curr_velocity"] (vx, vy, vz)
       # state["prev_angular_velocity"] = œâ_prev (scalar yaw rate)
       # state["curr_angular_velocity"] = œâ (yaw rate)
       # state["dt"] = timestep duration (e.g. 0.05 s)
       # state["max_lin_jerk"] = e.g. 0.5 m/s¬≥, state["max_ang_jerk"] = 0.5 rad/s¬≤
       lin_jerk = euclidean_distance(state["curr_velocity"], state["prev_velocity"]) / state["dt"]
       ang_diff = abs(state["curr_angular_velocity"] - state["prev_angular_velocity"])
       lin_penalty = min(1.0, lin_jerk / state["max_lin_jerk"] )
       ang_penalty = min(1.0, ang_diff / state["max_ang_jerk"] )
       # Reward ‚àà [0, 1], penalizing both linear and angular jerk equally
       return max(0.0, 1.0 - 0.5 * lin_penalty - 0.5 * ang_penalty)
   ```

6. **`be_fast_reward`**

   ```python
   def be_fast_reward(state, action):
       # state["time_elapsed"]       = seconds since episode start
       # state["max_time_allowed"]   = e.g. 30.0 s
       # state["curr_velocity"]      = (vx, vy, vz)
       # state["max_speed"]          = e.g. 1.5 m/s
       speed = euclidean_norm(state["curr_velocity"] )
       if state.get("at_goal", False):
           return 1.0 + (state["max_time_allowed"] - state["time_elapsed"]) / state["max_time_allowed"]
       else:
           # Provide a shaping reward based on forward speed toward goal
           return speed / state["max_speed"]
   ```

### Researcher Mode Additional Presets (3)

7. **`path_efficiency_reward`**

   ```python
   def path_efficiency_reward(state, action):
       # state["distance_traveled"]  = cumulative path length so far  
       # state["straight_line_dist"] = distance from start to goal  
       # state["prev_to_goal_dist"]  = previous step's distance to goal  
       # state["curr_to_goal_dist"]  = current step's distance to goal  
       if state.get("at_goal", False):
           eff = state["straight_line_dist"] / max(state["distance_traveled"], 1e-3)
           return eff   # ‚â•1 if path is longer than straight line; closer to 1 is better
       else:
           # Step-wise reward for moving closer to goal, normalized
           delta = state["prev_to_goal_dist"] - state["curr_to_goal_dist"]
           return delta / state["straight_line_dist"]
   ```

8. **`adaptive_disturbance_reward`**

   ```python
   def adaptive_disturbance_reward(state, action):
       # state["external_force"]    = (fx, fy, fz) on this timestep  
       # action["thrust_vector"]    = (tx, ty, tz) commanded thrust components  
       disturbance_mag = euclidean_norm(state["external_force"])
       comp_mag = projection_magnitude(action["thrust_vector"], state["external_force"])
       # Reward = how well the agent counters the disturbance, minus small penalty for magnitude
       return (comp_mag / (disturbance_mag + 1e-3)) - 0.1 * disturbance_mag
   ```

9. **`multi_objective_reward`**

   ```python
   def multi_objective_reward(state, action, weights):
       # weights = { "reach": w1, "collision": w2, "energy": w3, "speed": w4 }
       w1 = weights.get("reach", 1.0)
       w2 = weights.get("collision", 1.0)
       w3 = weights.get("energy", 1.0)
       w4 = weights.get("speed", 1.0)
       r_reach     = reach_target_reward(state, action)
       r_coll      = avoid_crashes_reward(state, action)
       r_energy    = save_energy_reward(state, action)
       # Step-wise speed component toward goal
       delta = state["prev_to_goal_dist"] - state["curr_to_goal_dist"]
       r_speed     = delta / state["straight_line_dist"] if state["straight_line_dist"] > 0 else 0
       # Weighted sum (ensure each sub-reward is ‚àà [0,1] or [‚Äì1,1])
       return w1 * r_reach + w2 * r_coll + w3 * r_energy + w4 * r_speed
   ```

---

*This completes the combined 12-week plan, the key parameter definitions, and all preset reward functions in a Markdown-ready format.*
