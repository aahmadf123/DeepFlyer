## 12-Week Summer Plan (May 14 â€“ July 27, 2025)

Below is a concise, Markdown-friendly table showing each team memberâ€™s responsibilities over the 12-week period. Roles: **Uma (Simulation & CAD)**, **Jay (UI & Backend Integration)**, **Ahmad [Me] (RL & AI)**.

| Week | Dates        | Uma: Simulation & CAD                                                                                                                                                 | Jay: UI & Backend Integration                                                                                                                                                                                                                                                 | Ahmad: RL & AI                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| :--: | :----------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|   1  | May 14â€“20    | â€¢ Set up ROSÂ 2 & Gazebo environment.<br>â€¢ Create bare-bones URDF visual model (no sensors).<br>â€¢ Verify drone spawns in Gazebo.                                       | â€¢ Scaffold FastAPI backend with placeholder endpoints.<br>â€¢ Initialize React/Next.js project skeleton.<br>â€¢ Define empty MongoDB schema.<br>â€¢ Create a stub â€œTrainingâ€ page.                                                                                                  | â€¢ Install Python, PyTorchÂ + CUDA, RL libraries.<br>â€¢ Clone simulation repo and verify ROSÂ 2 topics.<br>â€¢ Start a Python package for the RL agent with a basic PPO skeleton.<br>â€¢ Implement a â€œhello-worldâ€ loop sending zero velocities and logging status.                                                                                                                                                                                                                                                                                     |
|   2  | May 21â€“27    | â€¢ Add IMU & collision tags to URDF.<br>â€¢ Attach front-facing camera plugin (publish to `/drone/camera/front/image_raw`).<br>â€¢ Optimize meshes and validate in Gazebo. | â€¢ Expose `/api/rewards/list` and `/api/train/start`, `/api/train/status` (dummy).<br>â€¢ Build Mission Selector UI stub: dropdown for reward presets, â€œStart Trainingâ€ button.                                                                                                  | â€¢ Define six Explorer-mode reward function signatures: `reach_target`, `avoid_crashes`, `save_energy`, `fly_steady`, `fly_smoothly`, `be_fast`.<br>â€¢ Implement a `RewardRegistry` mapping IDsÂ â†’ functions.<br>â€¢ Expose `RewardRegistry.list_presets()` to FastAPI.                                                                                                                                                                                                                                                                              |
|   3  | May 28â€“JunÂ 3 | â€¢ Add motor plugins so drone can fly under velocity commands.<br>â€¢ Validate hover via Gazeboâ€™s GUI.                                                                   | â€¢ Populate dropdown by calling `/api/rewards/list`.<br>â€¢ On â€œStart Training,â€ POST preset & defaults to `/api/train/start`.<br>â€¢ Show spinner awaiting `/api/train/status`.<br>â€¢ Set up WebSocket skeleton for future streaming.                                              | â€¢ Implement baseline PPO training loop:<br>Â Â â€“ Subscribe to `/drone/odom` and `/drone/camera/front/image_raw`.<br>Â Â â€“ Publish actions to `/drone/cmd_vel`.<br>Â Â â€“ Reward = â€“distance\_to\_goal.<br>Â Â â€“ Update everyÂ 64 steps.<br>Â Â â€“ Log metrics to CSV and JSON status file.                                                                                                                                                                                                                                                                   |
|   4  | JunÂ 4â€“10     | â€¢ Test simple â€œmove to waypointâ€ scenario in Gazebo.<br>â€¢ Add downward-facing camera plugin for SLAM later.                                                           | â€¢ Connect `/api/train/start` to spawn background training job.<br>â€¢ Return `job_id` & set status to â€œstarted.â€<br>â€¢ Implement `/api/train/status` by reading JSON status file.                                                                                                | â€¢ Refactor to support `path_efficiency` reward.<br>â€¢ Accept `preset_id` from Jayâ€™s UI and use corresponding function.<br>â€¢ Validate that `path_efficiency_reward` changes logged rewards meaningfully.                                                                                                                                                                                                                                                                                                                                          |
|   5  | JunÂ 11â€“17    | â€¢ Integrate PX4/MAVROS so Gazebo simulates real autopilot behavior.<br>â€¢ Finalize Mission Selector UI readiness.                                                      | â€¢ Build â€œSimulation Viewerâ€: embed Gazebo camera feed via WebSocket or MJPEG.<br>â€¢ Add sliders for max velocity & acceleration, send to `/api/train/start`.                                                                                                                   | â€¢ Subscribe to downward camera, run simple SLAM (ORB-SLAM2 wrapper) to get altitude or 2D map.<br>â€¢ Implement `energy_efficiency_reward` penalizing throttle usage.<br>â€¢ Compare energy-efficiency vs. path-efficiency in small experiments.                                                                                                                                                                                                                                                                                                    |
|   6  | JunÂ 18â€“24    | â€¢ Debug MAVROS communication and data routing to ROS topics.<br>â€¢ Ensure collision detection works accurately.                                                        | â€¢ Integrate front camera stream into dashboard.<br>â€¢ Create a â€œTelemetry Overlayâ€ showing episode number, last reward, `preset_id`.<br>â€¢ Route hyperparameter sliders to `/api/train-start`.                                                                                  | â€¢ Route training parameters (e.g. collision penalty weight) to ROS topics so simulation can use them.<br>â€¢ Implement & validate `collision_avoidance_reward` and `fly_smoothly_reward`.<br>â€¢ Run brief tests to confirm correct behavior.                                                                                                                                                                                                                                                                                                       |
|   7  | JunÂ 25â€“JulÂ 1 | â€¢ Validate full URDF + PX4 + sensor stack under new presets.<br>â€¢ Build Gazebo worldÂ 2 (MapÂ 2: multi-path complex with dynamic obstacles).                            | â€¢ Add real-time charts: episode reward vs. episode, crash counts, variance.<br>â€¢ Enable side-by-side comparison of two training jobsâ€™ reward curves.                                                                                                                          | â€¢ Build interactive hyperparameter UI component:<br>Â Â â€“ Sliders for `learning_rate`, `gamma`, `entropy_coef`.<br>Â Â â€“ On change, send new values to `/api/train/start`.<br>â€¢ Implement a simple â€œauto-tune assistantâ€ that reviews reward plateau patterns and writes suggestions to a JSON file.                                                                                                                                                                                                                                                |
|   8  | JulÂ 2â€“8      | â€¢ Finish MapÂ 2: three route options, dynamic barriers, wind zones.<br>â€¢ Test SLAM & collision avoidance in MapÂ 2.                                                     | â€¢ Develop â€œEvaluation Dashboardâ€:<br>Â Â â€“ Graph: reward vs. episode with annotations.<br>Â Â â€“ Metrics: standard deviation of rewards, crashes per 10 episodes.<br>Â Â â€“ Flight-replay mini-map colored by speed or confusion.                                                     | â€¢ Integrate XAI overlays:<br>Â Â â€“ Compute Grad-CAM saliency from PPOâ€™s CNN layers.<br>Â Â â€“ Overlay onto camera stream and publish via WebSocket.<br>Â Â â€“ Allow toggling â€œShow Saliencyâ€ in UI.                                                                                                                                                                                                                                                                                                                                                     |
|   9  | JulÂ 9â€“15     | â€¢ Build MapÂ 3: multi-level complex (vertical levels, floating platforms, dynamic lighting).<br>â€¢ Validate SLAM & sensors on MapÂ 3.                                    | â€¢ Create an â€œRL Glossaryâ€ hover component: definitions for â€œepisode,â€ â€œreward,â€ â€œpolicy.â€<br>â€¢ Add tooltips next to hyperparameter sliders (e.g. explain â€œlearning rateâ€ in plain language).<br>â€¢ Integrate saliency stream so users can toggle it on/off.                    | â€¢ Expand RL tutorial overlays:<br>Â Â â€“ Show â€œAgent is exploringâ€ indicator when entropy high.<br>Â Â â€“ Refine AI Coach: analyze training logs (reward, loss, entropy) and trigger contextual tips (e.g. â€œlower learning rateâ€ or â€œreduce collisions at PlatformÂ 2â€).<br>Â Â â€“ Expose `/api/coach/suggestions` for Jayâ€™s UI.                                                                                                                                                                                                                          |
|  10  | JulÂ 16â€“22    | â€¢ Draft Curriculum Mode: three scenarios in sequence (MapsÂ 1 â†’Â 2 â†’Â 3).<br>â€¢ Configure Gazebo to load scenarios sequentially based on triggers.                        | â€¢ Build â€œScenario Creatorâ€ UI:<br>Â Â â€“ Drag & drop obstacles onto blank grid.<br>Â Â â€“ Set waypoints and checkpoints.<br>Â Â â€“ Save JSON to `/api/scenario/upload`.<br>â€¢ Create â€œCurriculumâ€ page where users pick scenario sequence.                                              | â€¢ Implement `ScenarioLoader`:<br>Â Â â€“ Read JSON from `/api/scenario/list`.<br>Â Â â€“ Spawn/Delete models via ROSÂ 2 services.<br>Â Â â€“ Load chosen scenario at training start.<br>â€¢ Create `CurriculumRunner` that:<br>Â Â 1. Trains on ScenarioÂ 1 until reward threshold.<br>Â Â 2. Saves model, loads ScenarioÂ 2, resumes training.<br>Â Â 3. Repeats for ScenarioÂ 3.<br>â€¢ Test end-to-end with `curriculum=true` flag.                                                                                                                                    |
|  11  | JulÂ 23â€“29    | â€¢ Validate multi-objective rewards in MapÂ 3 under Curriculum Mode.<br>â€¢ Add necessary sensors/triggers for multi-objective tasks.                                     | â€¢ Add UI controls for multi-objective weights:<br>Â Â â€“ Sliders for path\_efficiency, collision\_avoidance, energy\_saving, speed.<br>Â Â â€“ Send weight vector to `/api/train/start`.<br>â€¢ Build â€œCustom Reward Functionâ€ uploader: upload Python script, call Ahmadâ€™s validator. | â€¢ Implement `multi_objective_reward(state, action, weights)`:<br>Â Â â€“ Weighted sum of `reach_target`, `avoid_crashes`, `save_energy`, and step-wise speed.<br>Â Â â€“ Normalize each component.<br>â€¢ Implement `dynamic_mission_reward` to handle mid-mission goal changes.<br>â€¢ Write a â€œreward validatorâ€ script that:<br>Â Â 1. Loads the userâ€™s uploaded Python file and checks for signature `def custom_reward(state, action) -> float`.<br>Â Â 2. Runs a few dummy state/action tests.<br>Â Â 3. Returns pass/fail to Jayâ€™s `/api/reward/validate`. |
|  12  | JulÂ 30â€“AugÂ 5 | â€¢ Finalize domain randomization in Gazebo (vary wind, lighting, sensor noise).<br>â€¢ Create â€œSim-to-Realâ€ checklist PDF.                                               | â€¢ Build â€œSim-to-Realâ€ tutorial page:<br>Â Â â€“ Embed checklist steps.<br>Â Â â€“ Provide â€œDownload Modelâ€ button to fetch final ONNX file.<br>Â Â â€“ Offer â€œDry-Run in Simulationâ€ feature for final validation.                                                                        | â€¢ Complete domain randomization:<br>Â Â â€“ Randomize IMU noise, camera noise, minor force disturbances per episode.<br>Â Â â€“ Confirm models generalize across all three Gazebo worlds.<br>â€¢ Export final PPO model to ONNX.<br>â€¢ Write `sim_to_real_runner.py` that:<br>Â Â 1. Loads ONNX model.Â Â 2. Connects to real droneâ€™s ROSÂ 2 topics or staging simulator.Â Â 3. Streams telemetry and logs behavior differences.<br>â€¢ Deliver â€œSim-to-Realâ€ documentation for hardware team.                                                                      |

---

## Key Parameter Categories & Specific Parameters

Below are the **parameter categories** and **specific parameters** youâ€™ll need to define for an indoor-only RL setup.

### 1. Environment & Simulation Parameters

*(Primarily configured by Uma in Gazebo; Jay exposes toggles/fields but does not handle the physics directly.)*

* **Map Dimensions & Boundaries**

  * *Floor plan sizeÂ (X Ã— Y)*: 10Â m Ã—Â 10Â m
  * *Ceiling height*:Â 3Â m
  * *Obstacle density*:Â 0.1Â obs/mÂ²
  * *Obstacle shapes & positions*: e.g. (box, cylinder), coordinates in meters

* **Lighting Conditions**

  * *Ambient light intensity range*:Â 200â€“800Â lux
  * *Shadow variation*: boolean (on/off) or intensity parameter

* **Physics & Collision**

  * *Gravity*:Â 9.81Â m/sÂ²
  * *Air friction/damping*:Â drag coefficient 0.1â€“0.3
  * *Collision restitution (bounciness)*:Â 0.0 for hard collisions
  * *Floor friction coefficient*:Â 0.5

* **Wind & External Disturbances** (optional indoors)

  * *Wind gain*:Â 0 (no wind) or small gusts (0.1â€“0.3Â m/s)
  * *Random force noise magnitude*: Â±0.01Â N per timestep

* **Sensor Noise / Domain Randomization**

  * *Camera Gaussian noise Ïƒ*:Â e.g. 5Â intensity levels
  * *IMU noise floor*:Â accelÂ Ïƒ=0.02Â m/sÂ², gyroÂ Ïƒ=0.01Â rad/s
  * *Depth-sensor noise*:Â Â±0.05Â m
  * *Randomization ranges*: e.g. IMU noise âˆ¼Â U(0.01,Â 0.03)

### 2. State / Observation Parameters

*(Ahmadâ€™s RL code reads these; Uma publishes them; Jay may show them as â€œobservation infoâ€ in the UI.)*

* **Sensor Frame Rates**

  * *Camera FPS*:Â 15Â fps (front), 10Â fps (downward)
  * *IMU update rate*:Â 100Â Hz
  * *SLAM update rate*:Â 10Â Hz

* **Camera Resolution & Field of View**

  * *Resolution (W Ã— H)*:Â 640 Ã— 480â€¯pixels
  * *FOV horizontal*:Â 90Â°
  * *FOV vertical*:Â 60Â°

* **SLAM / Localization Outputs**

  * *Map resolution*:Â 0.05Â m per grid cell
  * *Pose noise threshold*:Â 0.1Â m
  * *Scan match tolerance*:Â 0.05Â m

* **Observation Vector Contents**

  * PositionÂ (x, y, z) and orientation (quaternion)
  * Linear & angular velocities (IMU data)
  * Depth or point-cloud slice from ToF/LiDAR
  * Front camera image (downsampled or grayscale)
  * Collision flag or distance\_to\_obstacle
  * Battery or energy estimate (optional)

### 3. Action / Control Parameters

*(Ahmadâ€™s RL loop writes these; Uma must accept them in Gazebo; Jay provides UI fields for end-users.)*

* **Velocity & Acceleration Limits**

  * *Max linear velocity (m/s)*:Â 1.5Â m/s
  * *Max linear acceleration (m/sÂ²)*:Â 2.0Â m/sÂ²
  * *Max angular velocity (rad/s)*:Â Ï€/2Â rad/s
  * *Max angular acceleration (rad/sÂ²)*:Â Ï€Â rad/sÂ²

* **Control Mode**

  * *Velocity vs. attitude commands*: use `/cmd_vel` (linearÂ & angular velocities)
  * *PID gains for velocity loop*:Â Kâ‚š=0.5,Â Káµ¢=0.1,Â Kğ’¹=0.05

* **Action Discretization vs. Continuous**

  * *Discrete example*:
    Â Â Â Â â€¢ 5 bins for linear x/y/z
    Â Â Â Â â€¢ 3 bins for yaw (left, none, right)
  * *Continuous example*:
    Â Â Â Â â€¢ Action vector âˆˆ â„â´:Â \[vâ‚“, váµ§, v\_z, Ï‰\_yaw], with bounds as above

### 4. Reward Function Parameters

*(Ahmadâ€™s RL code reads these; Jay shows sliders/inputs for user tuning; Uma ensures simulation publishes required state values.)*

* **Distance-to-Goal Component**

  * *Weightâ‚*:Â 1.0
  * *Goal tolerance radius*:Â 0.2Â m

* **Collision Penalty**

  * *Collision\_penalty*:Â â€“10.0
  * *Near-obstacle penalty scale*: â€“(1/dÂ²) if dÂ <Â 0.5Â m
  * *Near-miss threshold*:Â 0.2Â m (penalty â€“1.0)

* **Smoothness / Jerk Penalty**

  * *Weightâ‚‚*:Â 0.3
  * *Max\_lin\_jerk*:Â 0.5Â m/sÂ³
  * *Max\_ang\_jerk*:Â 0.5Â rad/sÂ²

* **Energy / Motor Usage Penalty**

  * *Weightâ‚ƒ*:Â 0.2
  * *Throttle threshold*:Â 0.7 (above means penalty)

* **Time / Completion Bonus**

  * *Time\_penalty\_rate*:Â â€“0.01 per timestep
  * *Completion\_bonus*:Â +5

* **Multi-Objective Weights**

  * *wâ‚Â (distance)*, *wâ‚‚Â (collision)*, *wâ‚ƒÂ (energy)*, *wâ‚„Â (speed)*: sliders in Researcher mode

### 5. RL Hyperparameters

*(Ahmad sets defaults; Jay provides sliders/fields for user tuning.)*

* **Learning Rate (Î±)**:Â 1e-5 â†’ 1e-2Â (defaultÂ 3e-4)
* **Discount Factor (Î³)**:Â 0.90 â†’ 0.999Â (defaultÂ 0.99)
* **Batch Size**: {32,Â 64,Â 128,Â 256}
* **Clip (PPO Îµ)**:Â 0.1 â†’Â 0.3Â (defaultÂ 0.2)
* **Entropy Coefficient**:Â 0.0 â†’Â 0.1Â (defaultÂ 0.01)
* **Value Loss Coefficient (Ï°\_v)**:Â 0.5 â†’Â 1.0Â (defaultÂ 0.5)
* **Epochs per Update**:Â 3 â†’Â 10Â (defaultÂ 5)
* **Max Gradient Norm**:Â 0.1 â†’Â 1.0Â (defaultÂ 0.5)
* **Replay Buffer Size**Â (if off-policy): 10kÂ â†’Â 100kÂ (defaultÂ 50k)

### 6. Training & Episode Parameters

*(Ahmadâ€™s code uses these; Jay provides fields.)*

* **Max Episodes**:Â 1000
* **Max Steps/Episode**:Â 500 (âˆ†t=0.05Â s, total 25Â s)
* **Warmâ€Up (random actions)**:Â 1000Â steps
* **Evaluation Frequency**:Â everyÂ 50Â episodesÂ (evaluateÂ 5Â episodes)
* **Checkpoint Frequency**:Â everyÂ 100Â episodes

### 7. Curriculum Mode Parameters

*(Researcher only; UI fields generate a JSON list.)*

* **Scenario Sequence**:Â \["map\_simple",Â "map\_complex",Â "map\_multilevel"]
* **Episode thresholds to progress**:Â average rewardÂ â‰¥Â 50 over lastÂ 20Â episodes
* **Reward thresholds**:Â \[30,Â 40,Â 50] per scenario
* **Reset condition**:Â 3 crashesÂ in a row

### 8. XAI & AI Coach Parameters

*(Researcher only; relevant to advanced users.)*

* **Saliency overlay threshold**:Â 0.6 activation
* **Coach plateau length**:Â 50 episodes with <5 reward improvement
* **Collision rate threshold**:Â >20% crashes over 20 episodes
* **Max suggestion frequency**:Â once everyÂ 20 episodes

---

## Preset Reward Functions

Below are the full definitions (in Pythonâ€style pseudocode) of each preset reward, grouped by mode.

### Explorer Mode (6 Presets)

1. **`reach_target_reward`**

   ```python
   def reach_target_reward(state, action):
       # state["position"] = (x, y, z)
       # state["goal"]     = (gx, gy, gz)
       # state["max_room_diagonal"] = diagonal length (precomputed)
       dist = euclidean_distance(state["position"], state["goal"])
       # Reward âˆˆ [0,Â 1], withÂ 1 at the goal, falls off linearly
       return max(0.0, 1.0 - (dist / state["max_room_diagonal"]))
   ```

2. **`avoid_crashes_reward`**

   ```python
   def avoid_crashes_reward(state, action):
       # state["collision_flag"]    = True/False
       # state["dist_to_obstacle"]  = nearest obstacle distance (m)
       if state["collision_flag"]:
           return -1.0  # heavy penalty for collision
       elif state["dist_to_obstacle"] < 0.2:
           return -0.5  # moderate penalty for very close
       else:
           return 0.0   # no penalty otherwise
   ```

3. **`save_energy_reward`**

   ```python
   def save_energy_reward(state, action):
       # action["throttle"] âˆˆ [0.0, 1.0]
       throttle = action["throttle"]
       # Reward is high if throttle is low; 1 (best) atÂ 0, 0 (worst) atÂ 1
       return 1.0 - throttle
   ```

4. **`fly_steady_reward`**

   ```python
   def fly_steady_reward(state, action):
       # state["altitude"]          = current z
       # state["target_altitude"]   = desired z
       # state["vertical_velocity"] = current vz
       # state["max_altitude_error"] = e.g. 1.0 m (normalization)
       alt_err = abs(state["altitude"] - state["target_altitude"])
       vz = abs(state["vertical_velocity"])
       # Baseline: 1.0 â€“ (alt_error normalized), then penalize vertical speed
       altitude_component = max(0.0, 1.0 - (alt_err / state["max_altitude_error"]))
       speed_penalty = 0.5 * vz
       return altitude_component - speed_penalty
   ```

5. **`fly_smoothly_reward`**

   ```python
   def fly_smoothly_reward(state, action):
       # state["prev_velocity"] (vx_prev, vy_prev, vz_prev)
       # state["curr_velocity"] (vx, vy, vz)
       # state["prev_angular_velocity"] = Ï‰_prev (scalar yaw rate)
       # state["curr_angular_velocity"] = Ï‰ (yaw rate)
       # state["dt"] = timestep duration (e.g. 0.05Â s)
       # state["max_lin_jerk"] = e.g. 0.5 m/sÂ³, state["max_ang_jerk"] = 0.5 rad/sÂ²
       lin_jerk = euclidean_distance(state["curr_velocity"], state["prev_velocity"]) / state["dt"]
       ang_diff = abs(state["curr_angular_velocity"] - state["prev_angular_velocity"])
       lin_penalty = min(1.0, lin_jerk / state["max_lin_jerk"] )
       ang_penalty = min(1.0, ang_diff / state["max_ang_jerk"] )
       # Reward âˆˆ [0,Â 1], penalizing both linear and angular jerk equally
       return max(0.0, 1.0 - 0.5 * lin_penalty - 0.5 * ang_penalty)
   ```

6. **`be_fast_reward`**

   ```python
   def be_fast_reward(state, action):
       # state["time_elapsed"]       = seconds since episode start
       # state["max_time_allowed"]   = e.g. 30.0 s
       # state["curr_velocity"]      = (vx, vy, vz)
       # state["max_speed"]          = e.g. 1.5 m/s
       speed = euclidean_norm(state["curr_velocity"] )
       if state.get("at_goal", False):
           return 1.0 + (state["max_time_allowed"] - state["time_elapsed"]) / state["max_time_allowed"]
       else:
           # Provide a shaping reward based on forward speed toward goal
           return speed / state["max_speed"]
   ```

### Researcher Mode Additional Presets (3)

7. **`path_efficiency_reward`**

   ```python
   def path_efficiency_reward(state, action):
       # state["distance_traveled"]  = cumulative path length so far  
       # state["straight_line_dist"] = distance from start to goal  
       # state["prev_to_goal_dist"]  = previous stepâ€™s distance to goal  
       # state["curr_to_goal_dist"]  = current stepâ€™s distance to goal  
       if state.get("at_goal", False):
           eff = state["straight_line_dist"] / max(state["distance_traveled"], 1e-3)
           return eff   # â‰¥1 if path is longer than straight line; closer toÂ 1 is better
       else:
           # Step-wise reward for moving closer to goal, normalized
           delta = state["prev_to_goal_dist"] - state["curr_to_goal_dist"]
           return delta / state["straight_line_dist"]
   ```

8. **`adaptive_disturbance_reward`**

   ```python
   def adaptive_disturbance_reward(state, action):
       # state["external_force"]    = (fx, fy, fz) on this timestep  
       # action["thrust_vector"]    = (tx, ty, tz) commanded thrust components  
       disturbance_mag = euclidean_norm(state["external_force"])
       comp_mag = projection_magnitude(action["thrust_vector"], state["external_force"])
       # Reward = how well the agent counters the disturbance, minus small penalty for magnitude
       return (comp_mag / (disturbance_mag + 1e-3)) - 0.1 * disturbance_mag
   ```

9. **`multi_objective_reward`**

   ```python
   def multi_objective_reward(state, action, weights):
       # weights = { "reach": w1, "collision": w2, "energy": w3, "speed": w4 }
       w1 = weights.get("reach", 1.0)
       w2 = weights.get("collision", 1.0)
       w3 = weights.get("energy", 1.0)
       w4 = weights.get("speed", 1.0)
       r_reach     = reach_target_reward(state, action)
       r_coll      = avoid_crashes_reward(state, action)
       r_energy    = save_energy_reward(state, action)
       # Step-wise speed component toward goal
       delta = state["prev_to_goal_dist"] - state["curr_to_goal_dist"]
       r_speed     = delta / state["straight_line_dist"] if state["straight_line_dist"] > 0 else 0
       # Weighted sum (ensure each sub-reward is âˆˆ [0,1] or [â€“1,1])
       return w1 * r_reach + w2 * r_coll + w3 * r_energy + w4 * r_speed
   ```

---

*This completes the combined 12-week plan, the key parameter definitions, and all preset reward functions in a Markdown-ready format.*
